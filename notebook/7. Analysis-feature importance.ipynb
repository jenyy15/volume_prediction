{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbccf572-c510-4d33-b1cb-4c46076a6011",
   "metadata": {},
   "source": [
    "### Analyze Results -2\n",
    "\n",
    "##### Check the feature importance of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab5bef57-48f5-4ca6-9a18-974158e59ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.model_training_utils import RNNDataFeeder, RnnAEDataFeeder, ModelConfig, read_data, load_data_columns_config\n",
    "from sys.model_analysis import load_model, plot_importance, save_plots_to_html\n",
    "from pmdarima.model_selection import RollingForecastCV, SlidingWindowForecastCV\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "config_dict = load_data_columns_config(version=2)\n",
    "factors_columns = ['tech_factors',\n",
    " 'calendar_factors',\n",
    " 'fundamental_factors',\n",
    " 'industry_factors',\n",
    " 'release_schedule_factors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad59ef0-f6e7-4489-a53d-64fe5de24bcc",
   "metadata": {},
   "source": [
    "#### [2]. Major contribution factor\n",
    "##### 1. Load data and prepare parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27989450-07a2-4d69-9d1d-98c35377cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the validation dataset\n",
    "# shuffle the k-th column value\n",
    "# make the prediction and test by metrics (MSE) again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce002c7-968b-498b-b12f-fa686af8c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "folder_path = \"F:/predictors_v2\"\n",
    "final_dataset = read_data(filename=\"final_dataset\", folder_path=folder_path)\n",
    "final_dataset = final_dataset.sort_values(by=[\"date\", \"isin\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad79dccf-0893-42dd-a31e-eec432df2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split train, validation (create cross validation spliter) and test datasets\n",
    "dates_list = final_dataset[\"date\"].unique()\n",
    "dates_list.sort()\n",
    "num_of_days = dates_list.shape[0]\n",
    "\n",
    "step = 60 # step in window movement\n",
    "h = 60 # time horizon for validation dataset\n",
    "trainval_test_threshold = int(num_of_days * 0.6) # 60% dates are used to training and validation\n",
    "initial_threshold = int(trainval_test_threshold / 3) # the window size of the 1st train dataset\n",
    "# Update the split threshold of train_validation and test\n",
    "trainval_test_threshold = (\n",
    "    (trainval_test_threshold - (initial_threshold + h)) // step * step\n",
    "    + h\n",
    "    + initial_threshold\n",
    ")\n",
    "# train_dates are the dates used for training and validation in models.\n",
    "train_dates = dates_list[:trainval_test_threshold]\n",
    "# Create cross validation spliter with sliding window (non-cumulative datasets)\n",
    "cv_spliter = SlidingWindowForecastCV(h=h, step=step, window_size=initial_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbcd5a6-aeed-4996-8c24-6f70da47f8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(2022, 3, 25), datetime.date(2023, 12, 29))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_list[trainval_test_threshold], dates_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7477e817-52a0-4ea7-96c5-a0c89dc5c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  2020-01-03 2020-10-16\n",
      "validation:  2020-10-19 2021-01-13\n",
      "train:  2020-03-31 2021-01-13\n",
      "validation:  2021-01-14 2021-04-12\n",
      "train:  2020-06-25 2021-04-12\n",
      "validation:  2021-04-13 2021-07-08\n",
      "train:  2020-09-21 2021-07-08\n",
      "validation:  2021-07-09 2021-10-01\n",
      "train:  2020-12-15 2021-10-01\n",
      "validation:  2021-10-04 2021-12-28\n",
      "train:  2021-03-15 2021-12-28\n",
      "validation:  2021-12-29 2022-03-24\n"
     ]
    }
   ],
   "source": [
    "for train, validation in cv_spliter.split(train_dates):\n",
    "    print(\"train: \", train_dates[train[0]], train_dates[train[-1]])\n",
    "    print(\"validation: \", train_dates[validation[0]], train_dates[validation[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a69089b-4e93-4fca-b0e4-6a1394853cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad44f77-7567-43a6-993e-562d3a24cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 849.08it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_lstm_encoder(win_size, predictors_size, latent_dim=50):\n",
    "    \"\"\"Create LSTM encoder\"\"\"\n",
    "    # Encoder\n",
    "    inputs = tf.keras.layers.Input(shape=(win_size, predictors_size))\n",
    "    # GRU Encoder Layer (bottleneck layer)\n",
    "    encoded = tf.keras.layers.LSTM(latent_dim, \n",
    "                                   recurrent_regularizer=tf.keras.regularizers.l1(0.0001),\n",
    "                                   return_sequences=False)(inputs)\n",
    "    # Repeat Latent Vector to match the original sequence length (needed for the decoder)\n",
    "    output = tf.keras.layers.RepeatVector(win_size)(encoded)\n",
    "    # Create the model\n",
    "    encoder = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def create_rnn_model(encoder, win_size, other_input_size):\n",
    "    \"\"\"transfer learning version\"\"\"\n",
    "    inputs = encoder.input\n",
    "    transfer_layer=encoder.output\n",
    "    if other_input_size>0:\n",
    "        inputs = tf.keras.layers.Input(shape=(win_size, other_input_size + encoder.input_shape[-1]),\n",
    "                                       name='full_input')\n",
    "        # Split dataset\n",
    "        other_input = tf.keras.layers.Lambda(lambda x: x[:, :, :other_input_size])(inputs)\n",
    "        # encoder datafeeder should be the last columns\n",
    "        encoder_input = tf.keras.layers.Lambda(lambda x: x[:, :, other_input_size:])(inputs)\n",
    "        encoder_output = encoder(encoder_input)\n",
    "        transfer_layer = tf.keras.layers.Concatenate()([other_input, encoder_output])\n",
    "    layer1 = tf.keras.layers.LSTM(32,\n",
    "                                  kernel_regularizer=None,\n",
    "                                  recurrent_regularizer=None)(transfer_layer)\n",
    "    layer2 = tf.keras.layers.Dense(16, activation='relu')(layer1)\n",
    "    layer3 = tf.keras.layers.Dense(8, activation='relu')(layer2)\n",
    "    output = tf.keras.layers.Dense(1)(layer3)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Only use the last validation split\n",
    "for _, test_idx in tqdm(cv_spliter.split(train_dates)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebb29e-da29-4305-9a38-ed4a05833186",
   "metadata": {},
   "source": [
    "##### 2. Analyze models with different predictor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ec171b-c5ac-4586-918d-2caecde5b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_dict={1: \"./checkpoints/lstm_l1_3_ae_test4_v6_CV6\", # tech+fun/trainable\n",
    "                      2: \"./checkpoints/lstm_l1_5_ae_v6_CV6\",       # all factors\n",
    "                      3: \"./checkpoints/lstm_l1_3_ae_test5_v6_CV6\"} # excluding industry factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4b2f2f-63d9-4a4e-b5df-e8b49f4f4ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 163s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1597 - r2: 0.9403\n",
      "128/128 [==============================] - 159s 1s/step - loss: 0.1503 - r2: 0.9438\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.1580 - r2: 0.9410\n",
      "128/128 [==============================] - 181s 1s/step - loss: 0.1499 - r2: 0.9440\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1609 - r2: 0.9399\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.1496 - r2: 0.9441\n",
      "128/128 [==============================] - 186s 1s/step - loss: 0.1495 - r2: 0.9441\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.1496 - r2: 0.9441\n",
      "128/128 [==============================] - 156s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 179s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 152s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 165s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 192s 2s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 170s 1s/step - loss: 0.1498 - r2: 0.9440\n",
      "128/128 [==============================] - 165s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 184s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 195s 2s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 162s 1s/step - loss: 0.1496 - r2: 0.9441\n",
      "128/128 [==============================] - 162s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 168s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 184s 1s/step - loss: 0.1498 - r2: 0.9440\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 165s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 193s 2s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 150s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 183s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 157s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 171s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 152s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.1498 - r2: 0.9441\n",
      "128/128 [==============================] - 173s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1496 - r2: 0.9441\n",
      "128/128 [==============================] - 140s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 151s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.1499 - r2: 0.9440\n",
      "128/128 [==============================] - 168s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 189s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.1498 - r2: 0.9440\n",
      "128/128 [==============================] - 160s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 178s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 159s 1s/step - loss: 0.1496 - r2: 0.9441\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 177s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 175s 1s/step - loss: 0.1498 - r2: 0.9441\n",
      "128/128 [==============================] - 151s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 177s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 139s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 176s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 182s 1s/step - loss: 0.1498 - r2: 0.9440\n",
      "128/128 [==============================] - 175s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 161s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 171s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 179s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 154s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1500 - r2: 0.9440\n",
      "128/128 [==============================] - 149s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 175s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 162s 1s/step - loss: 0.1506 - r2: 0.9437\n",
      "128/128 [==============================] - 161s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 179s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 157s 1s/step - loss: 0.1500 - r2: 0.9440\n",
      "128/128 [==============================] - 150s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 181s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 149s 1s/step - loss: 0.1498 - r2: 0.9440\n",
      "128/128 [==============================] - 155s 1s/step - loss: 0.1497 - r2: 0.9441\n",
      "128/128 [==============================] - 177s 1s/step - loss: 0.1497 - r2: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████▌                                      | 1/2 [3:42:50<3:42:50, 13370.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 133s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 139s 1s/step - loss: 0.1635 - r2: 0.9387\n",
      "128/128 [==============================] - 139s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 128s 994ms/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 161s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.6210 - r2: 0.7690\n",
      "128/128 [==============================] - 135s 1s/step - loss: 0.4474 - r2: 0.8333\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.6156 - r2: 0.7708\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.3417 - r2: 0.8726\n",
      "128/128 [==============================] - 126s 977ms/step - loss: 0.1631 - r2: 0.9388\n",
      "128/128 [==============================] - 132s 1s/step - loss: 0.1631 - r2: 0.9388\n",
      "128/128 [==============================] - 151s 1s/step - loss: 0.1631 - r2: 0.9388\n",
      "128/128 [==============================] - 134s 1s/step - loss: 0.1634 - r2: 0.9387\n",
      "128/128 [==============================] - 131s 1s/step - loss: 0.1633 - r2: 0.9388\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 133s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 141s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 166s 1s/step - loss: 0.1634 - r2: 0.9387\n",
      "128/128 [==============================] - 140s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 171s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 140s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 173s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 138s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 149s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 151s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 152s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 148s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 145s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 179s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 141s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1633 - r2: 0.9388\n",
      "128/128 [==============================] - 169s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 145s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 149s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 150s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 150s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 168s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 176s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 140s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 175s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.1633 - r2: 0.9387\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 166s 1s/step - loss: 0.1630 - r2: 0.9388\n",
      "128/128 [==============================] - 139s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 143s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 170s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 141s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 141s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 168s 1s/step - loss: 0.1630 - r2: 0.9388\n",
      "128/128 [==============================] - 138s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 168s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 148s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 137s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 158s 1s/step - loss: 0.1633 - r2: 0.9388\n",
      "128/128 [==============================] - 138s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 147s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 173s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 137s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 144s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 164s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 142s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 138s 1s/step - loss: 0.1631 - r2: 0.9388\n",
      "128/128 [==============================] - 153s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 151s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 136s 1s/step - loss: 0.1636 - r2: 0.9386\n",
      "128/128 [==============================] - 149s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 172s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 139s 1s/step - loss: 0.1635 - r2: 0.9386\n",
      "128/128 [==============================] - 146s 1s/step - loss: 0.1633 - r2: 0.9388\n",
      "128/128 [==============================] - 173s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 135s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 148s 1s/step - loss: 0.1632 - r2: 0.9388\n",
      "128/128 [==============================] - 179s 1s/step - loss: 0.1632 - r2: 0.9388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 2/2 [7:11:12<00:00, 12936.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the factor columns from config_dict\n",
    "factors_columns=['tech_factors', 'calendar_factors', 'fundamental_factors', \n",
    "                 'industry_factors', 'release_schedule_factors']\n",
    "\n",
    "def create_model(win_size, predictors_size, encoder_input_size=len(config_dict[\"fundamental_factors\"])):\n",
    "    \"\"\"Create Model based on encoder and LSTM model\"\"\"\n",
    "    encoder = create_lstm_encoder(win_size, encoder_input_size)\n",
    "    model = create_rnn_model(encoder, win_size, predictors_size-encoder_input_size)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_columns=config_dict['fundamental_factors']\n",
    "for path_num in tqdm([1,2,3]):\n",
    "    if path_num ==1:\n",
    "         data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    elif path_num ==2:\n",
    "         data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + \\\n",
    "        config_dict['industry_factors'] + config_dict['release_schedule_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    elif path_num ==3:\n",
    "         data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + \\\n",
    "        config_dict['release_schedule_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    \n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(data_columns)-3, \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "\n",
    "    checkpoint_path=checkpoint_path_dict[path_num]\n",
    "    model = load_model(data_feeder.window_size, data_feeder.predictors_size, \n",
    "                       checkpoint_path, create_model)\n",
    "    \n",
    "    val_filter = (data_feeder.predictors_dates>= train_dates[test_idx[0]]) & (\n",
    "        data_feeder.predictors_dates <= train_dates[test_idx[-1]]\n",
    "    )\n",
    "    \n",
    "    # Compute \"Permutation Feature Importance\"\n",
    "    # https://www.kaggle.com/code/cdeotte/lstm-feature-importance \n",
    "    # https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance\n",
    "    # runtime: 9.4 hours\n",
    "    fea_import_metrics={}\n",
    "    valid_ds = data_feeder.gen_tf_dataset(val_filter)\n",
    "    fea_import_metrics[\"baseline\"] = model.evaluate(valid_ds, verbose=1)\n",
    "    del valid_ds\n",
    "    tf.random.set_seed(4321)\n",
    "    start_i=74 if path_num==2 else 0\n",
    "    for i in range(start_i, data_feeder.predictors_size):\n",
    "        valid_ds_i = data_feeder.gen_tf_dataset(subset_filter=val_filter, column_idx=i)\n",
    "        fea_import_metrics[data_columns[i+2]] = model.evaluate(valid_ds_i)\n",
    "        del valid_ds_i\n",
    "    # Save the metric of this model\n",
    "    with open(f\"./metrics/fea_import_lstm_final_pathnum{path_num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(fea_import_metrics, pickle_file)\n",
    "    del data_feeder, model, fea_import_metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f62f285-adc2-4165-bb7b-065bcbb4ea04",
   "metadata": {},
   "source": [
    "create_model(data_feeder.window_size, data_feeder.predictors_size, encoder_input_size=len(config_dict[\"fundamental_factors\"])).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8348dd2-08ce-418f-b957-5d52a23301b0",
   "metadata": {},
   "source": [
    "##### 2. DISPLAY LSTM FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65ae49a-14f4-497b-ab83-b575acace62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys.model_analysis import plot_importance, save_plots_to_html, load_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50171946-70ff-42b8-8fb8-0e0be3d9f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. plot version:\n",
    "factor_num_dict={1: \"tech&fundamental\",\n",
    "                 2: \"tech&calendar&fundamental&industry&release\",\n",
    "                 3: \"tech&calendar&fundamental&release\"}\n",
    "\n",
    "plots = []\n",
    "for path_num in [1,2,3]:\n",
    "    fig = plot_importance(metrics_path=f\"./metrics/fea_import_lstm_final_pathnum{path_num}.pkl\",\n",
    "                          factor=factor_num_dict[path_num], num=path_num)\n",
    "    plots.append(fig)\n",
    "    \n",
    "save_plots_to_html(figures=plots, filename=\"./final_feature_importance_plots.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b4aace6-a8ae-4e63-984b-374920e9353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1:  tech&fundamental\n",
      "baseline:  [0.1632276177406311, 0.938768744468689]\n",
      "                       MSE        R2  sqrt_MSE_per_change  abs_sqrt_MSE_per_change\n",
      "lag_logvol_ma1    0.620996  0.768985             0.950508                 0.950508\n",
      "lag_logvol_ma22   0.615579  0.770771             0.941981                 0.941981\n",
      "lag_logvol_ma5    0.447396  0.833275             0.655577                 0.655577\n",
      "lag_logvol_ma252  0.341693  0.872590             0.446842                 0.446842\n",
      "\n",
      "3:  tech&calendar&fundamental&release\n",
      "baseline:  [0.15162834525108337, 0.9434125423431396]\n",
      "                       MSE        R2  sqrt_MSE_per_change  abs_sqrt_MSE_per_change\n",
      "lag_logvol_ma5    0.808872  0.699419             1.309669                 1.309669\n",
      "lag_logvol_ma1    0.567780  0.789116             0.935085                 0.935085\n",
      "lag_logvol_ma252  0.450066  0.832505             0.722852                 0.722852\n",
      "lag_logvol_ma22   0.263879  0.901694             0.319204                 0.319204\n",
      "≥3                0.166290  0.937975             0.047233                 0.047233\n",
      "≤-3               0.165222  0.938346             0.043865                 0.043865\n",
      "0                 0.160349  0.940212             0.028356                 0.028356\n",
      "on_after_witch    0.152525  0.943063             0.002954                 0.002954\n",
      "lag_SIZE_ma1      0.152500  0.943087             0.002871                 0.002871\n",
      "lag_STREVRSL_ma1  0.152492  0.943088             0.002845                 0.002845\n",
      "-1_2              0.152325  0.943155             0.002295                 0.002295\n",
      "lag_RESVOL_ma1    0.151965  0.943285             0.001110                 0.001110\n",
      "\n",
      "2:  tech&calendar&fundamental&industry&release\n",
      "baseline:  [0.14969348907470703, 0.9440810680389404]\n",
      "                       MSE        R2  sqrt_MSE_per_change  abs_sqrt_MSE_per_change\n",
      "lag_logvol_ma1    0.699280  0.740121             1.161345                 1.161345\n",
      "lag_logvol_ma5    0.637855  0.762859             1.064236                 1.064236\n",
      "lag_logvol_ma22   0.351519  0.869171             0.532404                 0.532404\n",
      "lag_logvol_ma252  0.316763  0.882059             0.454674                 0.454674\n",
      "≥3                0.160857  0.939941             0.036618                 0.036618\n",
      "≤-3               0.159728  0.940331             0.032974                 0.032974\n",
      "0                 0.157995  0.941031             0.027355                 0.027355\n",
      "lag_SIZE_ma1      0.150646  0.943728             0.003176                 0.003176\n",
      "-1_2              0.150338  0.943844             0.002149                 0.002149\n",
      "on_after_witch    0.150305  0.943841             0.002041                 0.002041\n",
      "lag_BIOLIFE       0.150224  0.943887             0.001769                 0.001769\n",
      "lag_SHORTINT_ma1  0.149999  0.943969             0.001020                 0.001020\n"
     ]
    }
   ],
   "source": [
    "# 2. numerical version:\n",
    "for path_num in [1,3,2]:\n",
    "    test5_dict=load_metrics(f\"./metrics/fea_import_lstm_final_pathnum{path_num}.pkl\")\n",
    "    test5_df = pd.DataFrame(test5_dict, index=[\"MSE\", \"R2\"]).T\n",
    "    print(f\"\\n{path_num}: \", factor_num_dict[path_num])\n",
    "    print(\"baseline: \", test5_df.loc[\"baseline\"].tolist())\n",
    "    test5_df['sqrt_MSE_per_change'] = np.sqrt(test5_df['MSE'])/np.sqrt(test5_df.loc[\"baseline\", 'MSE'])-1\n",
    "    \n",
    "    test5_df[\"abs_sqrt_MSE_per_change\"]=test5_df['sqrt_MSE_per_change'].abs()\n",
    "    print(test5_df[test5_df[\"abs_sqrt_MSE_per_change\"]>0.001].sort_values(by=[\"abs_sqrt_MSE_per_change\"], ascending=False).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
