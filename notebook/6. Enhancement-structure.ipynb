{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ed8d13-629f-41a7-b56f-18fb92715430",
   "metadata": {},
   "source": [
    "### Rerun the models with model structure enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b86ecd19-7b5e-4f36-b5d0-8ce1164acc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices are already configured, skipping setup.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.model_training_utils import RNNDataFeeder, RnnAEDataFeeder, ModelConfig, read_data, load_data_columns_config\n",
    "from src.train_models import setup_gpu, fit_models_with_cross_validation, fit_models_with_cross_validation_v2, update_pretrain_filter, train_NN\n",
    "from pmdarima.model_selection import RollingForecastCV, SlidingWindowForecastCV\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup GUP within this script\n",
    "setup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60a126-dea1-4bbf-b8d2-38dad50eabea",
   "metadata": {},
   "source": [
    "### [1]. Preparation\n",
    "Prepare the parameters for the model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed68f6f5-300d-43db-9f3b-ca307c2dcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "folder_path = \"F:/predictors_v2\"\n",
    "# pre_train_dataset\n",
    "pre_train_dataset = read_data(filename=\"pre_train_dataset\", folder_path=folder_path)\n",
    "pre_train_dataset = pre_train_dataset.sort_values(by=[\"date\", \"isin\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34f839b0-fc44-42c8-8690-c4cfae5d4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: set parameters for models \n",
    "# predictors_size: the number of predictors\n",
    "# win_size (for RNN models, win_size>=1): the window size of each data point\n",
    "\n",
    "def create_lstm_autoencoder(win_size, predictors_size, latent_dim=50):\n",
    "    # Encoder\n",
    "    inputs = tf.keras.layers.Input(shape=(win_size, predictors_size))\n",
    "    # GRU Encoder Layer (bottleneck layer)\n",
    "    encoded = tf.keras.layers.LSTM(latent_dim, \n",
    "                                   recurrent_regularizer=tf.keras.regularizers.l1(0.0001),\n",
    "                                   return_sequences=False)(inputs)\n",
    "    # Repeat Latent Vector to match the original sequence length (needed for the decoder)\n",
    "    decoder_input = tf.keras.layers.RepeatVector(win_size)(encoded)\n",
    "    # GRU Decoder Layer to reconstruct the input sequence\n",
    "    decoder = tf.keras.layers.LSTM(predictors_size, return_sequences=True)(decoder_input)\n",
    "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(predictors_size))(decoder)\n",
    "    # Create the model\n",
    "    autoencoder = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "def create_rnn_model(encoder, win_size, other_input_size):\n",
    "    \"\"\"transfer learning version\"\"\"\n",
    "    inputs = encoder.input\n",
    "    transfer_layer=encoder.output\n",
    "    if other_input_size>0:\n",
    "        inputs = tf.keras.layers.Input(shape=(win_size, other_input_size + encoder.input_shape[-1]),\n",
    "                                       name='full_input')\n",
    "        # Split dataset\n",
    "        other_input = tf.keras.layers.Lambda(lambda x: x[:, :, :other_input_size])(inputs)\n",
    "        # encoder datafeeder should be the last columns\n",
    "        encoder_input = tf.keras.layers.Lambda(lambda x: x[:, :, other_input_size:])(inputs)\n",
    "        encoder_output = encoder(encoder_input)\n",
    "        transfer_layer = tf.keras.layers.Concatenate()([other_input, encoder_output])\n",
    "    layer1 = tf.keras.layers.LSTM(32,\n",
    "                                  kernel_regularizer=None,\n",
    "                                  recurrent_regularizer=None)(transfer_layer)\n",
    "    layer2 = tf.keras.layers.Dense(16, activation='relu')(layer1)\n",
    "    layer3 = tf.keras.layers.Dense(8, activation='relu')(layer2)\n",
    "    output = tf.keras.layers.Dense(1)(layer3)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "def create_rnn_model_normal(win_size, predictors_size):\n",
    "    inputs = tf.keras.layers.Input(shape=(win_size,predictors_size))\n",
    "    layer1 = tf.keras.layers.LSTM(32,\n",
    "                                  kernel_regularizer=None,\n",
    "                                  recurrent_regularizer=None)(inputs)\n",
    "    layer2 = tf.keras.layers.Dense(16, activation='relu')(layer1)\n",
    "    layer3 = tf.keras.layers.Dense(8, activation='relu')(layer2)\n",
    "    output = tf.keras.layers.Dense(1)(layer3)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60fefe2-3f0f-444b-be2a-02d725c3ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_dict has keys:  dict_keys(['index_columns', 'tech_factors', 'calendar_factors', 'fundamental_factors', 'industry_factors', 'agg_industry_factors', 'release_schedule_factors', 'output_columns'])\n"
     ]
    }
   ],
   "source": [
    "# Load new config\n",
    "config_dict = load_data_columns_config(version=2)\n",
    "# Show the content of config_dict:\n",
    "print(\"config_dict has keys: \", config_dict.keys())\n",
    "\n",
    "final_dataset = read_data(filename=\"final_dataset\",\n",
    "                          folder_path=folder_path)\n",
    "final_dataset = final_dataset.sort_values(by=[\"date\", \"isin\"], ignore_index=True)\n",
    "\n",
    "\n",
    "# Get the factor columns from config_dict\n",
    "factors_columns=['tech_factors', 'calendar_factors', 'fundamental_factors', \n",
    "                 'industry_factors', 'release_schedule_factors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c12f558-ee25-4a90-832c-26d0b5e36cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split train, validation (create cross validation spliter) and test datasets\n",
    "dates_list = final_dataset[\"date\"].unique()\n",
    "dates_list.sort()\n",
    "num_of_days = dates_list.shape[0]\n",
    "\n",
    "step = 60 # step in window movement\n",
    "h = 60 # time horizon for validation dataset\n",
    "trainval_test_threshold = int(num_of_days * 0.6) # 60% dates are used to training and validation\n",
    "initial_threshold = int(trainval_test_threshold / 3) # the window size of the 1st train dataset\n",
    "# Update the split threshold of train_validation and test\n",
    "trainval_test_threshold = (\n",
    "    (trainval_test_threshold - (initial_threshold + h)) // step * step\n",
    "    + h\n",
    "    + initial_threshold\n",
    ")\n",
    "# train_dates are the dates used for training and validation in models.\n",
    "train_dates = dates_list[:trainval_test_threshold]\n",
    "# test_dates are the dates used for testing (out-of-sample datasets)\n",
    "test_dates = dates_list[trainval_test_threshold:]\n",
    "# Create the test_filter, an input for model training.\n",
    "test_filter = (final_dataset[\"date\"] >= test_dates[0]) & (\n",
    "    final_dataset[\"date\"] <= test_dates[-1]\n",
    ")\n",
    "# Create cross validation spliter with sliding window (non-cumulative datasets)\n",
    "cv_spliter = SlidingWindowForecastCV(h=h, step=step, window_size=initial_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae228f73-969a-40e6-b825-6a39b1cababe",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_columns=['tech_factors', 'calendar_factors', 'fundamental_factors', \n",
    "                 'industry_factors', 'release_schedule_factors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1f9e4-733e-4cb8-8cee-38fb5978a313",
   "metadata": {},
   "source": [
    "### [2]. Train models\n",
    "##### 1. Fit models by both predictor and model struture enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822896d1-bb83-4d0a-9c03-e2ba8179cda5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_columns = list(pre_train_dataset.columns[2:])\n",
    "for num in [3, 4, 5]:\n",
    "    # The data feeder of RNN autoencoder models:\n",
    "    # (1). it needs ISIN  and date columns. The 1st 2 columns of data_df are [\"date\", \"isin\"].\n",
    "    # (2). the data_df should be sorted by [\"date\", \"isin\"]\n",
    "    # (5). it supports window_size >= 1\n",
    "    ae_data_feeder = RnnAEDataFeeder(data_df=pre_train_dataset, \n",
    "                                     window_size=10, \n",
    "                                     batch_size=1024,\n",
    "                                     predictors_size = len(input_columns), \n",
    "                                     predictors_dates=pre_train_dataset['date'])\n",
    "    \n",
    "    \n",
    "    if num==3:\n",
    "        data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + \\\n",
    "        input_columns + [\"log_adj_volume\"]\n",
    "    elif num==4:\n",
    "        data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + \\\n",
    "        config_dict['industry_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    else:\n",
    "        data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + \\\n",
    "        config_dict['industry_factors'] + config_dict['release_schedule_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    # The data feeder of RNN models:\n",
    "    # (1). it needs ISIN  and date columns. The 1st 2 columns of data_df are [\"date\", \"isin\"].\n",
    "    # (2). the data_df should be sorted by [\"date\", \"isin\"]\n",
    "    # (3). the last column of data_df should be the response variable column name\n",
    "    # (4). it only supports response variable in shape of (1, 1)\n",
    "    # (5). it supports window_size >= 1\n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(data_columns)-3, \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "    # The configuration of the model:\n",
    "    # (1). model_name: the name of the model\n",
    "    # (2). create_dense_model: a function to generate a model structure\n",
    "    # (3). other parameters: \n",
    "    #      verbose: verbose during model training \n",
    "    #      lr: learning rate\n",
    "    model_config = ModelConfig(model_name=f\"lstm_l1_{num}_ae_v4\", # rerun using v6 tag\n",
    "                               model_structure=create_rnn_model, \n",
    "                               verbose=0, lr=0.001)\n",
    "    # model configuration of the pretrain autoencoder\n",
    "    ae_model_config = ModelConfig(model_name=f\"ae_lstm_l1_{num}_ae_v4\",  # rerun using v6 tag\n",
    "                                  model_structure=create_lstm_autoencoder, \n",
    "                                  verbose=0, lr=0.001, encoder_trainable=True)\n",
    "    \n",
    "    tf.random.set_seed(1234)\n",
    "    # To train models with cross validation, early stopping and learning rate reducer \n",
    "    # runtime: 4 hs 25 mins\n",
    "    train_metrics_dict, test_metrics, ae_metrics = fit_models_with_cross_validation_v2(\n",
    "        data_feeder=data_feeder,\n",
    "        ae_data_feeder=ae_data_feeder,\n",
    "        cv_spliter=cv_spliter,\n",
    "        train_dates=train_dates,\n",
    "        test_filter=test_filter.values, # test_filter should be a numpy array\n",
    "        model_config=model_config,\n",
    "        ae_model_config=ae_model_config,\n",
    "        model_name=f\"lstm_l1_v4_ae_{num}\" # rerun using v6 tag\n",
    "    )\n",
    "    del train_metrics_dict, test_metrics, ae_metrics, ae_data_feeder, data_feeder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a74d5f-91be-40e1-b750-00093d4b458e",
   "metadata": {},
   "source": [
    "##### 2. Fit models to test...\n",
    "(1). trainable vs non-trainable <br>\n",
    "(2). other factor sets combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65716cc-ec2f-42d0-bc08-2f9f21121233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_l1_3_ae_test2_v6: tech+cal+fun/ encoder trainable\n",
    "# lstm_l1_3_ae_test3_v6: tech+fun/ encoder non-trainable\n",
    "# lstm_l1_3_ae_test4_v6: tech+fun/ encoder trainable\n",
    "# lstm_l1_3_ae_test5_v6: tech+cal+fun+release/ encoder trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262227d0-2ee4-4508-9e67-808112e63faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-03 2020-01-02 len=252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-01 2020-03-30 len=252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-26 2020-06-24 len=252\n",
      "2019-09-20 2020-09-18 len=252\n",
      "1118/1118 [==============================] - 1530s 1s/step - loss: 0.1764 - r2: 0.9308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:06:58, 978.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-16 2020-12-14 len=252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [1:07:00, 670.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-13 2021-03-12 len=252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num=3\n",
    "\n",
    "for test_num in [2,3,4,5]:\n",
    "    input_columns = list(pre_train_dataset.columns[2:])\n",
    "    # The data feeder of RNN autoencoder models:\n",
    "    # (1). it needs ISIN  and date columns. The 1st 2 columns of data_df are [\"date\", \"isin\"].\n",
    "    # (2). the data_df should be sorted by [\"date\", \"isin\"]\n",
    "    # (5). it supports window_size >= 1\n",
    "    ae_data_feeder = RnnAEDataFeeder(data_df=pre_train_dataset, \n",
    "                                     window_size=10, \n",
    "                                     batch_size=1024,\n",
    "                                     predictors_size = len(input_columns), \n",
    "                                     predictors_dates=pre_train_dataset['date'])\n",
    "    \n",
    "    if test_num ==5:\n",
    "        data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + \\\n",
    "        config_dict['release_schedule_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    elif test_num ==2:\n",
    "        data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + config_dict['calendar_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    elif test_num in [3,4]:\n",
    "        data_columns = [\"date\", \"isin\"] + config_dict['tech_factors'] + input_columns + [\"log_adj_volume\"]\n",
    "    # The data feeder of RNN models:\n",
    "    # (1). it needs ISIN  and date columns. The 1st 2 columns of data_df are [\"date\", \"isin\"].\n",
    "    # (2). the data_df should be sorted by [\"date\", \"isin\"]\n",
    "    # (3). the last column of data_df should be the response variable column name\n",
    "    # (4). it only supports response variable in shape of (1, 1)\n",
    "    # (5). it supports window_size >= 1\n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(data_columns)-3, \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "    # The configuration of the model:\n",
    "    # (1). model_name: the name of the model\n",
    "    # (2). create_dense_model: a function to generate a model structure\n",
    "    # (3). other parameters: \n",
    "    #      verbose: verbose during model training \n",
    "    #      lr: learning rate\n",
    "    model_config = ModelConfig(model_name=f\"lstm_l1_{num}_ae_test{test_num}_v6_3\", \n",
    "                               model_structure=create_rnn_model, \n",
    "                               verbose=0, lr=0.001)\n",
    "    \n",
    "    # model configuration of the pretrain autoencoder\n",
    "    if test_num == 3:\n",
    "        ae_model_config = ModelConfig(model_name=f\"ae_lstm_l1_{num}_ae_test{test_num}_v6_3\", \n",
    "                                      model_structure=create_lstm_autoencoder, \n",
    "                                      verbose=0, lr=0.001, encoder_trainable=False)\n",
    "    else:\n",
    "        ae_model_config = ModelConfig(model_name=f\"ae_lstm_l1_{num}_ae_test{test_num}_v6_3\", \n",
    "                                      model_structure=create_lstm_autoencoder, \n",
    "                                      verbose=0, lr=0.001, encoder_trainable=True)\n",
    "    \n",
    "    tf.random.set_seed(1234)\n",
    "    # To train models with cross validation, early stopping and learning rate reducer \n",
    "    # runtime: 4 hs 25 mins\n",
    "    train_metrics_dict, test_metrics, ae_metrics = fit_models_with_cross_validation_v2(\n",
    "        data_feeder=data_feeder,\n",
    "        ae_data_feeder=ae_data_feeder,\n",
    "        cv_spliter=cv_spliter,\n",
    "        train_dates=train_dates,\n",
    "        test_filter=test_filter.values, # test_filter should be a numpy array\n",
    "        model_config=model_config,\n",
    "        ae_model_config=ae_model_config,\n",
    "        model_name=f\"lstm_l1_v6_ae_test{test_num}_{num}\",\n",
    "        skip_cv_list=None,\n",
    "    )\n",
    "    del train_metrics_dict, test_metrics, ae_metrics, ae_data_feeder, data_feeder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
