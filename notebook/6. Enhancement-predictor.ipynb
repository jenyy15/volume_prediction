{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f02775a-ac12-4066-a804-784c27a487c3",
   "metadata": {},
   "source": [
    "### Rerun the models with predictor enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b70cd-89d3-45b0-b69f-8ba21a6d4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.model_training_utils import RNNDataFeeder, RnnAEDataFeeder, ModelConfig, read_data, load_data_columns_config\n",
    "from src.train_models import setup_gpu, fit_models_with_cross_validation, fit_models_with_cross_validation_v2\n",
    "from pmdarima.model_selection import RollingForecastCV, SlidingWindowForecastCV\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup GUP within this script\n",
    "setup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641c84a-8016-4a22-8c06-cc8964983fd0",
   "metadata": {},
   "source": [
    "### [1]. Preparation\n",
    "Prepare the parameters for the model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed68f6f5-300d-43db-9f3b-ca307c2dcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "folder_path = \"F:/predictors_v2\"\n",
    "# pre_train_dataset\n",
    "pre_train_dataset = read_data(filename=\"pre_train_dataset\", folder_path=folder_path)\n",
    "pre_train_dataset = pre_train_dataset.sort_values(by=[\"date\", \"isin\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58676c3-9bd7-4df6-9d52-d9cb1bbb9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: set parameters for models \n",
    "# predictors_size: the number of predictors\n",
    "# win_size (for RNN models, win_size>=1): the window size of each data point\n",
    "\n",
    "def create_rnn_model_normal(win_size, predictors_size):\n",
    "    inputs = tf.keras.layers.Input(shape=(win_size,predictors_size))\n",
    "    layer1 = tf.keras.layers.LSTM(32,\n",
    "                                  kernel_regularizer=None,\n",
    "                                  recurrent_regularizer=None)(inputs)\n",
    "    layer2 = tf.keras.layers.Dense(16, activation='relu')(layer1)\n",
    "    layer3 = tf.keras.layers.Dense(8, activation='relu')(layer2)\n",
    "    output = tf.keras.layers.Dense(1)(layer3)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd7dcda-b2f2-4f59-b4a2-f1710973dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_dict has keys:  dict_keys(['index_columns', 'tech_factors', 'calendar_factors', 'fundamental_factors', 'industry_factors', 'agg_industry_factors', 'release_schedule_factors', 'output_columns'])\n"
     ]
    }
   ],
   "source": [
    "# Load new config\n",
    "config_dict = load_data_columns_config(version=2)\n",
    "# Show the content of config_dict:\n",
    "print(\"config_dict has keys: \", config_dict.keys())\n",
    "\n",
    "final_dataset = read_data(filename=\"final_dataset\",\n",
    "                          folder_path=folder_path)\n",
    "final_dataset = final_dataset.sort_values(by=[\"date\", \"isin\"], ignore_index=True)\n",
    "\n",
    "# Get the factor columns from config_dict\n",
    "# v2\n",
    "factors_columns=['tech_factors', 'calendar_factors', 'fundamental_factors', \n",
    "                 'industry_factors', 'release_schedule_factors']\n",
    "# v3\n",
    "# factors_columns=['tech_factors', 'calendar_factors', 'fundamental_factors', \n",
    "#                  'agg_industry_factors', 'release_schedule_factors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "708abc17-c962-4004-a97a-a22890586649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split train, validation (create cross validation spliter) and test datasets\n",
    "dates_list = final_dataset[\"date\"].unique()\n",
    "dates_list.sort()\n",
    "num_of_days = dates_list.shape[0]\n",
    "\n",
    "step = 60 # step in window movement\n",
    "h = 60 # time horizon for validation dataset\n",
    "trainval_test_threshold = int(num_of_days * 0.6) # 60% dates are used to training and validation\n",
    "initial_threshold = int(trainval_test_threshold / 3) # the window size of the 1st train dataset\n",
    "# Update the split threshold of train_validation and test\n",
    "trainval_test_threshold = (\n",
    "    (trainval_test_threshold - (initial_threshold + h)) // step * step\n",
    "    + h\n",
    "    + initial_threshold\n",
    ")\n",
    "# train_dates are the dates used for training and validation in models.\n",
    "train_dates = dates_list[:trainval_test_threshold]\n",
    "# test_dates are the dates used for testing (out-of-sample datasets)\n",
    "test_dates = dates_list[trainval_test_threshold:]\n",
    "# Create the test_filter, an input for model training.\n",
    "test_filter = (final_dataset[\"date\"] >= test_dates[0]) & (\n",
    "    final_dataset[\"date\"] <= test_dates[-1]\n",
    ")\n",
    "# Create cross validation spliter with sliding window (non-cumulative datasets)\n",
    "cv_spliter = SlidingWindowForecastCV(h=h, step=step, window_size=initial_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1e7be-0bb5-4f74-b796-89a48b9a7be9",
   "metadata": {},
   "source": [
    "### [2]. Train models\n",
    "##### 1. Fit models by cumulatively adding predictor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a123edf0-b366-4fb4-9e96-0f7a3aae848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 430s 385ms/step - loss: 0.2024 - r2: 0.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [14:56, 896.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 426s 381ms/step - loss: 0.1847 - r2: 0.9277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [31:50, 965.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 436s 390ms/step - loss: 0.1794 - r2: 0.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [45:27, 898.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 441s 395ms/step - loss: 0.1827 - r2: 0.9286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:06:23, 1039.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 444s 397ms/step - loss: 0.1888 - r2: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:23:30, 1034.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 454s 406ms/step - loss: 0.1831 - r2: 0.9286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [1:40:16, 1002.73s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 485s 434ms/step - loss: 0.1875 - r2: 0.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [15:15, 915.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 467s 417ms/step - loss: 0.1722 - r2: 0.9324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [33:10, 1009.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 461s 412ms/step - loss: 0.1724 - r2: 0.9326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [48:36, 971.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 457s 408ms/step - loss: 0.1747 - r2: 0.9316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:06:21, 1008.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 465s 416ms/step - loss: 0.1789 - r2: 0.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:24:14, 1031.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 465s 416ms/step - loss: 0.1716 - r2: 0.9329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [1:41:38, 1016.36s/it]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(1234)\n",
    "\n",
    "num=1\n",
    "input_columns=[]\n",
    "for factor in factors_columns:\n",
    "    input_columns = input_columns + config_dict[factor]\n",
    "    data_columns = [\"date\", \"isin\"] + input_columns + [\"log_adj_volume\"]\n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(input_columns), \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "\n",
    "    model_config = ModelConfig(model_name=f\"lstm_{num}_tp_v2\", \n",
    "                               model_structure=create_rnn_model_normal, \n",
    "                               verbose=0, lr=0.001)\n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "    # To train models with cross validation, early stopping and learning rate reduc er \n",
    "    # runtime: 1:55\n",
    "    train_metrics_dict, test_metrics = fit_models_with_cross_validation(\n",
    "        data_feeder=data_feeder,\n",
    "        cv_spliter=cv_spliter,\n",
    "        train_dates=train_dates,\n",
    "        test_filter=test_filter.values, # test_filter should be a numpy array\n",
    "        model_config=model_config,\n",
    "        model_name=f\"lstm_v2_{num}\"\n",
    "    )\n",
    "    # Save the metric of this model\n",
    "    with open(f\"./metrics/train_metrics_dict_lstm_v2_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(train_metrics_dict, pickle_file)\n",
    "    with open(f\"./metrics/test_metrics_lstm_v2_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(test_metrics, pickle_file)       \n",
    "    # Release the memory space\n",
    "    del train_metrics_dict, test_metrics, data_feeder, model_config\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c40809-4653-4092-b58d-a13a8cd539dd",
   "metadata": {},
   "source": [
    "##### 2. Fit models by each predictor set (factors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4c7662-517d-45ef-af1f-f2bee09ffcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 227s 203ms/step - loss: 2.2577 - r2: 0.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [08:55, 535.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 220s 197ms/step - loss: 2.1969 - r2: 0.1504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [17:38, 528.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 222s 199ms/step - loss: 2.0931 - r2: 0.1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [26:20, 525.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 223s 200ms/step - loss: 2.0428 - r2: 0.2106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [35:06, 525.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 227s 203ms/step - loss: 2.1101 - r2: 0.1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [43:50, 525.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 230s 205ms/step - loss: 2.0294 - r2: 0.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [52:41, 526.88s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 1/1 [52:41<00:00, 3161.59s/it]\n"
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "# runtime: 6.5 hours\n",
    "for factor in tqdm(factors_columns):\n",
    "    # input_columns are the column names of predcitors in this model\n",
    "    input_columns = config_dict[factor]\n",
    "    data_columns = [\"date\", \"isin\"] + input_columns + [\"log_adj_volume\"]\n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(input_columns), \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "\n",
    "    model_config = ModelConfig(model_name=f\"lstm_{num}_tp_sc_v2\", \n",
    "                               model_structure=create_rnn_model_normal, \n",
    "                               verbose=0, lr=0.001) \n",
    "\n",
    "    tf.random.set_seed(1234)\n",
    "    # To train models with cross validation, early stopping and learning rate reducer \n",
    "    train_metrics_dict, test_metrics = fit_models_with_cross_validation(\n",
    "        data_feeder=data_feeder,\n",
    "        cv_spliter=cv_spliter,\n",
    "        train_dates=train_dates,\n",
    "        test_filter=test_filter.values, # test_filter should be a numpy array\n",
    "        model_config=model_config,\n",
    "        model_name=f\"lstm_single_cate_v2_{num}\" # lstm_v2_{num}\n",
    "    )\n",
    "    # Save the metric of this model\n",
    "    with open(f\"./metrics/train_metrics_dict_lstm_single_cate_v2_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(train_metrics_dict, pickle_file)\n",
    "    with open(f\"./metrics/test_metrics_lstm_single_cate_v2_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(test_metrics, pickle_file)       \n",
    "    # Release the memory space\n",
    "    del train_metrics_dict, test_metrics, data_feeder, model_config\n",
    "    num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
