{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acce6449-2b33-4292-a910-494de7171cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices are already configured, skipping setup.\n"
     ]
    }
   ],
   "source": [
    "from model_training_utils import RNNDataFeeder, ModelConfig, read_data, load_data_columns_config\n",
    "from train_models import fit_models_with_cross_validation, make_plot\n",
    "from pmdarima.model_selection import RollingForecastCV, SlidingWindowForecastCV\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f815ab1-6b63-4252-975a-5608b15137aa",
   "metadata": {},
   "source": [
    "### Target:\n",
    "##### [1]. Preparation\n",
    "Prepare the parameters for RNN models building and training\n",
    "##### [2]. Train models\n",
    "1. Fit models by cumulatively adding predictor sets\n",
    "2. Fit models by each predictor set (factors_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac743ee-f380-496e-9688-818b56460883",
   "metadata": {},
   "source": [
    "### [1]. Preparation\n",
    "Prepare the parameters for the model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72869f24-fdf7-4fc8-b687-06431b77d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_dict has keys:  dict_keys(['index_columns', 'tech_factors', 'calendar_factors', 'fundamental_factors', 'industry_factors', 'release_schedule_factors', 'output_columns'])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data\n",
    "folder_path = \"F:/predictors\"\n",
    "final_dataset = read_data(filename=\"final_dataset\", folder_path=folder_path).drop(columns=[\"lag_≥5\"])\n",
    "final_dataset = final_dataset.sort_values(by=[\"date\", \"isin\"], ignore_index=True)\n",
    "\n",
    "config_dict = load_data_columns_config()\n",
    "\n",
    "# Show the content of config_dict:\n",
    "print(\"config_dict has keys: \", config_dict.keys())\n",
    "\n",
    "# Get the factor columns from config_dict\n",
    "factors_columns=['tech_factors', 'calendar_factors', 'fundamental_factors', \n",
    "                 'industry_factors', 'release_schedule_factors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962c1741-e784-4cf4-a2c0-a3a5d0754958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split train, validation (create cross validation spliter) and test datasets\n",
    "dates_list = final_dataset[\"date\"].unique()\n",
    "dates_list.sort()\n",
    "num_of_days = dates_list.shape[0]\n",
    "\n",
    "step = 60 # step in window movement\n",
    "h = 60 # time horizon for validation dataset\n",
    "trainval_test_threshold = int(num_of_days * 0.6) # 60% dates are used to training and validation\n",
    "initial_threshold = int(trainval_test_threshold / 3) # the window size of the 1st train dataset\n",
    "# Update the split threshold of train_validation and test\n",
    "trainval_test_threshold = (\n",
    "    (trainval_test_threshold - (initial_threshold + h)) // step * step\n",
    "    + h\n",
    "    + initial_threshold\n",
    ")\n",
    "# train_dates are the dates used for training and validation in models.\n",
    "train_dates = dates_list[:trainval_test_threshold]\n",
    "# test_dates are the dates used for testing (out-of-sample datasets)\n",
    "test_dates = dates_list[trainval_test_threshold:]\n",
    "# Create the test_filter, an input for model training.\n",
    "test_filter = (final_dataset[\"date\"] >= test_dates[0]) & (\n",
    "    final_dataset[\"date\"] <= test_dates[-1]\n",
    ")\n",
    "# Create cross validation spliter with sliding window (non-cumulative datasets)\n",
    "cv_spliter = SlidingWindowForecastCV(h=h, step=step, window_size=initial_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e64a85a-a2a7-4b5b-aee9-52e156f0a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: set parameters for models \n",
    "# predictors_size: the number of predictors\n",
    "# win_size (for RNN models, win_size>=1): the window size of each data point\n",
    "\n",
    "def create_rnn_model(win_size, predictors_size):\n",
    "    inputs = tf.keras.layers.Input(shape=(win_size,predictors_size))\n",
    "    layer1 = tf.keras.layers.LSTM(32,\n",
    "                                  kernel_regularizer=None,\n",
    "                                  recurrent_regularizer=None)(inputs)\n",
    "    layer2 = tf.keras.layers.Dense(16, activation='relu')(layer1)\n",
    "    layer3 = tf.keras.layers.Dense(8, activation='relu')(layer2)\n",
    "    output = tf.keras.layers.Dense(1)(layer3)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69ef1165-2861-4d6c-a9ac-5ac21cca92c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10, 104)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                17536     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                528       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,209\n",
      "Trainable params: 18,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# sample dense model structure:\n",
    "create_rnn_model(10,104).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d306df-b530-494f-be9a-3b88f857d441",
   "metadata": {},
   "source": [
    "### [2]. Train models\n",
    "##### 1. Fit models by cumulatively adding predictor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b24e76-aff0-4cbe-81dc-0afd6388c751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Train models\n",
    "# test how the different groups of features have impact on the \n",
    "input_columns = ['isin']\n",
    "num = 1\n",
    "# runtime: ~ 17.5 hours\n",
    "for factor in tqdm(factors_columns):\n",
    "    # input_columns are \"isin\" + the column names of predcitors in this model\n",
    "    if num == 5:\n",
    "        input_columns = input_columns + config_dict[factor][:-1]\n",
    "    else:\n",
    "        input_columns = input_columns + config_dict[factor]\n",
    "    # data_columns includes \"date\" + input_columns and response variable column name\n",
    "    data_columns = [\"date\"] + input_columns + [\"log_adj_volume\"]\n",
    "    # The data feeder of RNN models:\n",
    "    # (1). it needs ISIN  and date columns. The 1st 2 columns of data_df are [\"date\", \"isin\"].\n",
    "    # (2). the data_df should be sorted by [\"date\", \"isin\"]\n",
    "    # (3). the last column of data_df should be the response variable column name\n",
    "    # (4). it only supports response variable in shape of (1, 1)\n",
    "    # (5). it supports window_size >= 1\n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(input_columns)-1, \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "    # The configuration of the model:\n",
    "    # (1). model_name: the name of the model\n",
    "    # (2). create_dense_model: a function to generate a model structure\n",
    "    # (3). other parameters: \n",
    "    #      verbose: verbose during model training \n",
    "    #      lr: learning rate\n",
    "    model_config = ModelConfig(model_name=f\"lstm_{num}_tp\", \n",
    "                               model_structure=create_rnn_model, \n",
    "                               verbose=0, lr=0.001)\n",
    "    \n",
    "    # Set seed for reproducing the result\n",
    "    tf.random.set_seed(1234)\n",
    "    # To train models with cross validation, early stopping and learning rate reducer \n",
    "    train_metrics_dict, test_metrics = fit_models_with_cross_validation(\n",
    "        data_feeder=data_feeder,\n",
    "        cv_spliter=cv_spliter,\n",
    "        train_dates=train_dates,\n",
    "        test_filter=test_filter.values, # test_filter should be a numpy array\n",
    "        model_config=model_config,\n",
    "        model_name=f\"lstm_{num}\"\n",
    "    )\n",
    "    # Save the metric of this model\n",
    "    with open(f\"./metrics/train_metrics_dict_lstm_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(train_metrics_dict, pickle_file)\n",
    "    with open(f\"./metrics/test_metrics_lstm_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(test_metrics, pickle_file)       \n",
    "    # Release the memory space\n",
    "    del train_metrics_dict, test_metrics, data_feeder, model_config\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea87d5a-7a7f-4a95-8f06-56ba94612de1",
   "metadata": {},
   "source": [
    "##### 2. Fit models by each predictor set (factors_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "678bf6fb-c511-47c0-9583-8c833e4b4756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 555s 496ms/step - loss: 2.1713 - r2: 0.1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [23:29, 1409.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 556s 497ms/step - loss: 2.1069 - r2: 0.1861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [44:07, 1308.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 551s 493ms/step - loss: 2.0890 - r2: 0.1921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [1:04:43, 1275.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 574s 514ms/step - loss: 1.9986 - r2: 0.2281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [1:27:35, 1313.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 558s 499ms/step - loss: 2.1101 - r2: 0.1841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [1:48:14, 1286.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 565s 505ms/step - loss: 1.9552 - r2: 0.2449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [2:08:45, 1287.52s/it]\n",
      " 50%|███████████████████████████████████████                                       | 1/2 [2:08:46<2:08:46, 7726.25s/it]\n",
      "\u001b[A [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 544s 487ms/step - loss: 2.7445 - r2: -0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [16:29, 989.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 545s 488ms/step - loss: 2.6582 - r2: -0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [32:56, 987.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 547s 489ms/step - loss: 2.6570 - r2: -0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [54:26, 1125.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 541s 484ms/step - loss: 2.6630 - r2: -0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [1:16:16, 1198.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 574s 514ms/step - loss: 2.6338 - r2: -0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A [1:33:10, 1132.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118/1118 [==============================] - 574s 514ms/step - loss: 2.6238 - r2: -0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [1:50:31, 1105.21s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 2/2 [3:59:17<00:00, 7178.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Fit models by each predictor set (factors_columns)\n",
    "# runtime: ~5.5 hours\n",
    "num = 2\n",
    "# rerun num=3 the last cv\n",
    "for factor in tqdm(factors_columns[1:]):\n",
    "    # input_columns are \"isin\" + the column names of predcitors in this model\n",
    "    if num == 5:\n",
    "        input_columns = ['isin'] + config_dict[factor][:-1]\n",
    "    else:\n",
    "        input_columns = ['isin'] + config_dict[factor]\n",
    "    # data_columns includes input_columns and response variable column name\n",
    "    data_columns = [\"date\"] + input_columns + [\"log_adj_volume\"]\n",
    "    \n",
    "    data_feeder = RNNDataFeeder(data_df=final_dataset[data_columns], \n",
    "                                window_size=10, \n",
    "                                batch_size=1024,\n",
    "                                predictors_size = len(input_columns)-1, \n",
    "                                predictors_dates=final_dataset['date'])\n",
    "    model_config = ModelConfig(model_name=f\"lstm_{num}_tp_sc\", \n",
    "                               model_structure=create_rnn_model, \n",
    "                               verbose=0, lr=0.001)\n",
    "    \n",
    "    # Set seed for reproducing the result\n",
    "    tf.random.set_seed(1234)\n",
    "    # To train models with cross validation, early stopping and learning rate reducer \n",
    "    train_metrics_dict, test_metrics = fit_models_with_cross_validation(\n",
    "        data_feeder=data_feeder,\n",
    "        cv_spliter=cv_spliter,\n",
    "        train_dates=train_dates,\n",
    "        test_filter=test_filter.values, # test_filter should be a numpy array\n",
    "        model_config=model_config,\n",
    "        model_name=f\"lstm_single_cate_{num}\"\n",
    "    )\n",
    "    # Save the metric of this model\n",
    "    with open(f\"./metrics/train_metrics_dict_lstm_single_cate_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(train_metrics_dict, pickle_file)\n",
    "    with open(f\"./metrics/test_metrics_lstm_single_cate_{num}.pkl\", \"wb\") as pickle_file:\n",
    "        pickle.dump(test_metrics, pickle_file)       \n",
    "    num += 1\n",
    "    # Release the memory space\n",
    "    del train_metrics_dict, test_metrics, data_feeder, model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c4dea-6819-413f-ac69-e72dd4bb54c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
